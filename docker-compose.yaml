x-app: &app
  env_file:
    - .env
  restart: unless-stopped
  platform: linux/amd64

services:
  postgres:
    image: postgres:15-alpine
    container_name: learning-postgres
    <<: *app
    volumes:
      - postgres_volume:/var/lib/postgresql/data
      - ./back/postgres:/docker-entrypoint-initdb.d
    ports:
      - "8305:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-разработка}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: learning-redis
    <<: *app
    ports:
      - "8306:6379"
    volumes:
      - redis_volume:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  elasticsearch:
    image: elasticsearch:8.13.0
    container_name: learning-elasticsearch
    <<: *app
    environment:
      discovery.type: single-node
      xpack.security.enabled: 'false'
      ES_JAVA_OPTS: -Xms512m -Xmx1g
      # Для разработки - отключаем некоторые проверки
      bootstrap.memory_lock: 'false'
    ports:
      - "8307:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-webserver:
    image: apache/airflow:2.9.1-python3.11
    container_name: learning-airflow-webserver
    <<: *app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./back/airflow/dags:/opt/airflow/dags
      - ./back/airflow/logs:/opt/airflow/logs
      - ./back/airflow/plugins:/opt/airflow/plugins
      - ./back/airflow/init.sh:/opt/airflow/init.sh
    ports:
      - "8308:8080"
    entrypoint: /opt/airflow/init.sh
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.9.1-python3.11
    container_name: learning-airflow-scheduler
    <<: *app
    depends_on:
      - airflow-webserver
    volumes:
      - ./back/airflow/dags:/opt/airflow/dags
      - ./back/airflow/logs:/opt/airflow/logs
      - ./back/airflow/plugins:/opt/airflow/plugins
      - ./back/airflow/init.sh:/opt/airflow/init.sh
    entrypoint: /opt/airflow/init.sh
    command: scheduler

  backend:
    build:
      context: ./back
    container_name: learning-backend
    <<: *app
    volumes:
      - ./back:/usr/src/app
    ports:
      - "8301:8000"  # Learning API порт
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    environment:
      - PYTHONPATH=/usr/src/app
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - ENVIRONMENT=development
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload --log-level info

  frontend:
    build:
      context: ./front
      args:
        - VITE_LOGGING=${VITE_LOGGING:-info}
        - VITE_BACKEND_URL=${VITE_BACKEND_URL:-http://localhost:8301}
    <<: *app
    container_name: learning-frontend
    volumes:
      - ./front/app:/app
      - frontend_node_modules:/app/node_modules  # Кеш node_modules
    ports:
      - "8302:5173"  # Learning Vite dev server
      - "8303:80"    # Learning Nginx production
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://backend:8000

volumes:
  postgres_volume:
    driver: local
  redis_volume:
    driver: local
  es_data:
    driver: local
  frontend_node_modules:
    driver: local
